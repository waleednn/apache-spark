== Physical Plan ==
* BroadcastNestedLoopJoin Inner BuildRight (182)
:- * BroadcastNestedLoopJoin Inner BuildRight (160)
:  :- * BroadcastNestedLoopJoin Inner BuildRight (138)
:  :  :- * BroadcastNestedLoopJoin Inner BuildRight (116)
:  :  :  :- * BroadcastNestedLoopJoin Inner BuildRight (94)
:  :  :  :  :- * BroadcastNestedLoopJoin Inner BuildRight (72)
:  :  :  :  :  :- * BroadcastNestedLoopJoin Inner BuildRight (50)
:  :  :  :  :  :  :- * HashAggregate (28)
:  :  :  :  :  :  :  +- Exchange (27)
:  :  :  :  :  :  :     +- * HashAggregate (26)
:  :  :  :  :  :  :        +- * Project (25)
:  :  :  :  :  :  :           +- * BroadcastHashJoin Inner BuildRight (24)
:  :  :  :  :  :  :              :- * Project (18)
:  :  :  :  :  :  :              :  +- * BroadcastHashJoin Inner BuildRight (17)
:  :  :  :  :  :  :              :     :- * Project (11)
:  :  :  :  :  :  :              :     :  +- * BroadcastHashJoin Inner BuildRight (10)
:  :  :  :  :  :  :              :     :     :- * Project (4)
:  :  :  :  :  :  :              :     :     :  +- * Filter (3)
:  :  :  :  :  :  :              :     :     :     +- * ColumnarToRow (2)
:  :  :  :  :  :  :              :     :     :        +- Scan parquet spark_catalog.default.store_sales (1)
:  :  :  :  :  :  :              :     :     +- BroadcastExchange (9)
:  :  :  :  :  :  :              :     :        +- * Project (8)
:  :  :  :  :  :  :              :     :           +- * Filter (7)
:  :  :  :  :  :  :              :     :              +- * ColumnarToRow (6)
:  :  :  :  :  :  :              :     :                 +- Scan parquet spark_catalog.default.store (5)
:  :  :  :  :  :  :              :     +- BroadcastExchange (16)
:  :  :  :  :  :  :              :        +- * Project (15)
:  :  :  :  :  :  :              :           +- * Filter (14)
:  :  :  :  :  :  :              :              +- * ColumnarToRow (13)
:  :  :  :  :  :  :              :                 +- Scan parquet spark_catalog.default.household_demographics (12)
:  :  :  :  :  :  :              +- BroadcastExchange (23)
:  :  :  :  :  :  :                 +- * Project (22)
:  :  :  :  :  :  :                    +- * Filter (21)
:  :  :  :  :  :  :                       +- * ColumnarToRow (20)
:  :  :  :  :  :  :                          +- Scan parquet spark_catalog.default.time_dim (19)
:  :  :  :  :  :  +- BroadcastExchange (49)
:  :  :  :  :  :     +- * HashAggregate (48)
:  :  :  :  :  :        +- Exchange (47)
:  :  :  :  :  :           +- * HashAggregate (46)
:  :  :  :  :  :              +- * Project (45)
:  :  :  :  :  :                 +- * BroadcastHashJoin Inner BuildRight (44)
:  :  :  :  :  :                    :- * Project (38)
:  :  :  :  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (37)
:  :  :  :  :  :                    :     :- * Project (35)
:  :  :  :  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (34)
:  :  :  :  :  :                    :     :     :- * Project (32)
:  :  :  :  :  :                    :     :     :  +- * Filter (31)
:  :  :  :  :  :                    :     :     :     +- * ColumnarToRow (30)
:  :  :  :  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (29)
:  :  :  :  :  :                    :     :     +- ReusedExchange (33)
:  :  :  :  :  :                    :     +- ReusedExchange (36)
:  :  :  :  :  :                    +- BroadcastExchange (43)
:  :  :  :  :  :                       +- * Project (42)
:  :  :  :  :  :                          +- * Filter (41)
:  :  :  :  :  :                             +- * ColumnarToRow (40)
:  :  :  :  :  :                                +- Scan parquet spark_catalog.default.time_dim (39)
:  :  :  :  :  +- BroadcastExchange (71)
:  :  :  :  :     +- * HashAggregate (70)
:  :  :  :  :        +- Exchange (69)
:  :  :  :  :           +- * HashAggregate (68)
:  :  :  :  :              +- * Project (67)
:  :  :  :  :                 +- * BroadcastHashJoin Inner BuildRight (66)
:  :  :  :  :                    :- * Project (60)
:  :  :  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (59)
:  :  :  :  :                    :     :- * Project (57)
:  :  :  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (56)
:  :  :  :  :                    :     :     :- * Project (54)
:  :  :  :  :                    :     :     :  +- * Filter (53)
:  :  :  :  :                    :     :     :     +- * ColumnarToRow (52)
:  :  :  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (51)
:  :  :  :  :                    :     :     +- ReusedExchange (55)
:  :  :  :  :                    :     +- ReusedExchange (58)
:  :  :  :  :                    +- BroadcastExchange (65)
:  :  :  :  :                       +- * Project (64)
:  :  :  :  :                          +- * Filter (63)
:  :  :  :  :                             +- * ColumnarToRow (62)
:  :  :  :  :                                +- Scan parquet spark_catalog.default.time_dim (61)
:  :  :  :  +- BroadcastExchange (93)
:  :  :  :     +- * HashAggregate (92)
:  :  :  :        +- Exchange (91)
:  :  :  :           +- * HashAggregate (90)
:  :  :  :              +- * Project (89)
:  :  :  :                 +- * BroadcastHashJoin Inner BuildRight (88)
:  :  :  :                    :- * Project (82)
:  :  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (81)
:  :  :  :                    :     :- * Project (79)
:  :  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (78)
:  :  :  :                    :     :     :- * Project (76)
:  :  :  :                    :     :     :  +- * Filter (75)
:  :  :  :                    :     :     :     +- * ColumnarToRow (74)
:  :  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (73)
:  :  :  :                    :     :     +- ReusedExchange (77)
:  :  :  :                    :     +- ReusedExchange (80)
:  :  :  :                    +- BroadcastExchange (87)
:  :  :  :                       +- * Project (86)
:  :  :  :                          +- * Filter (85)
:  :  :  :                             +- * ColumnarToRow (84)
:  :  :  :                                +- Scan parquet spark_catalog.default.time_dim (83)
:  :  :  +- BroadcastExchange (115)
:  :  :     +- * HashAggregate (114)
:  :  :        +- Exchange (113)
:  :  :           +- * HashAggregate (112)
:  :  :              +- * Project (111)
:  :  :                 +- * BroadcastHashJoin Inner BuildRight (110)
:  :  :                    :- * Project (104)
:  :  :                    :  +- * BroadcastHashJoin Inner BuildRight (103)
:  :  :                    :     :- * Project (101)
:  :  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (100)
:  :  :                    :     :     :- * Project (98)
:  :  :                    :     :     :  +- * Filter (97)
:  :  :                    :     :     :     +- * ColumnarToRow (96)
:  :  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (95)
:  :  :                    :     :     +- ReusedExchange (99)
:  :  :                    :     +- ReusedExchange (102)
:  :  :                    +- BroadcastExchange (109)
:  :  :                       +- * Project (108)
:  :  :                          +- * Filter (107)
:  :  :                             +- * ColumnarToRow (106)
:  :  :                                +- Scan parquet spark_catalog.default.time_dim (105)
:  :  +- BroadcastExchange (137)
:  :     +- * HashAggregate (136)
:  :        +- Exchange (135)
:  :           +- * HashAggregate (134)
:  :              +- * Project (133)
:  :                 +- * BroadcastHashJoin Inner BuildRight (132)
:  :                    :- * Project (126)
:  :                    :  +- * BroadcastHashJoin Inner BuildRight (125)
:  :                    :     :- * Project (123)
:  :                    :     :  +- * BroadcastHashJoin Inner BuildRight (122)
:  :                    :     :     :- * Project (120)
:  :                    :     :     :  +- * Filter (119)
:  :                    :     :     :     +- * ColumnarToRow (118)
:  :                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (117)
:  :                    :     :     +- ReusedExchange (121)
:  :                    :     +- ReusedExchange (124)
:  :                    +- BroadcastExchange (131)
:  :                       +- * Project (130)
:  :                          +- * Filter (129)
:  :                             +- * ColumnarToRow (128)
:  :                                +- Scan parquet spark_catalog.default.time_dim (127)
:  +- BroadcastExchange (159)
:     +- * HashAggregate (158)
:        +- Exchange (157)
:           +- * HashAggregate (156)
:              +- * Project (155)
:                 +- * BroadcastHashJoin Inner BuildRight (154)
:                    :- * Project (148)
:                    :  +- * BroadcastHashJoin Inner BuildRight (147)
:                    :     :- * Project (145)
:                    :     :  +- * BroadcastHashJoin Inner BuildRight (144)
:                    :     :     :- * Project (142)
:                    :     :     :  +- * Filter (141)
:                    :     :     :     +- * ColumnarToRow (140)
:                    :     :     :        +- Scan parquet spark_catalog.default.store_sales (139)
:                    :     :     +- ReusedExchange (143)
:                    :     +- ReusedExchange (146)
:                    +- BroadcastExchange (153)
:                       +- * Project (152)
:                          +- * Filter (151)
:                             +- * ColumnarToRow (150)
:                                +- Scan parquet spark_catalog.default.time_dim (149)
+- BroadcastExchange (181)
   +- * HashAggregate (180)
      +- Exchange (179)
         +- * HashAggregate (178)
            +- * Project (177)
               +- * BroadcastHashJoin Inner BuildRight (176)
                  :- * Project (170)
                  :  +- * BroadcastHashJoin Inner BuildRight (169)
                  :     :- * Project (167)
                  :     :  +- * BroadcastHashJoin Inner BuildRight (166)
                  :     :     :- * Project (164)
                  :     :     :  +- * Filter (163)
                  :     :     :     +- * ColumnarToRow (162)
                  :     :     :        +- Scan parquet spark_catalog.default.store_sales (161)
                  :     :     +- ReusedExchange (165)
                  :     +- ReusedExchange (168)
                  +- BroadcastExchange (175)
                     +- * Project (174)
                        +- * Filter (173)
                           +- * ColumnarToRow (172)
                              +- Scan parquet spark_catalog.default.time_dim (171)


(1) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(2) ColumnarToRow [codegen id : 4]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]

(3) Filter [codegen id : 4]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]
Condition : ((isnotnull(ss_hdemo_sk#2) AND isnotnull(ss_sold_time_sk#1)) AND isnotnull(ss_store_sk#3))

(4) Project [codegen id : 4]
Output [3]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, ss_sold_date_sk#4]

(5) Scan parquet spark_catalog.default.store
Output [2]: [s_store_sk#5, s_store_name#6]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store]
PushedFilters: [IsNotNull(s_store_name), EqualTo(s_store_name,ese), IsNotNull(s_store_sk)]
ReadSchema: struct<s_store_sk:int,s_store_name:string>

(6) ColumnarToRow [codegen id : 1]
Input [2]: [s_store_sk#5, s_store_name#6]

(7) Filter [codegen id : 1]
Input [2]: [s_store_sk#5, s_store_name#6]
Condition : ((isnotnull(s_store_name#6) AND (s_store_name#6 = ese)) AND isnotnull(s_store_sk#5))

(8) Project [codegen id : 1]
Output [1]: [s_store_sk#5]
Input [2]: [s_store_sk#5, s_store_name#6]

(9) BroadcastExchange
Input [1]: [s_store_sk#5]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=1]

(10) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_store_sk#3]
Right keys [1]: [s_store_sk#5]
Join type: Inner
Join condition: None

(11) Project [codegen id : 4]
Output [2]: [ss_sold_time_sk#1, ss_hdemo_sk#2]
Input [4]: [ss_sold_time_sk#1, ss_hdemo_sk#2, ss_store_sk#3, s_store_sk#5]

(12) Scan parquet spark_catalog.default.household_demographics
Output [3]: [hd_demo_sk#7, hd_dep_count#8, hd_vehicle_count#9]
Batched: true
Location [not included in comparison]/{warehouse_dir}/household_demographics]
PushedFilters: [Or(Or(And(EqualTo(hd_dep_count,4),LessThanOrEqual(hd_vehicle_count,6)),And(EqualTo(hd_dep_count,2),LessThanOrEqual(hd_vehicle_count,4))),And(EqualTo(hd_dep_count,0),LessThanOrEqual(hd_vehicle_count,2))), IsNotNull(hd_demo_sk)]
ReadSchema: struct<hd_demo_sk:int,hd_dep_count:int,hd_vehicle_count:int>

(13) ColumnarToRow [codegen id : 2]
Input [3]: [hd_demo_sk#7, hd_dep_count#8, hd_vehicle_count#9]

(14) Filter [codegen id : 2]
Input [3]: [hd_demo_sk#7, hd_dep_count#8, hd_vehicle_count#9]
Condition : (((((hd_dep_count#8 = 4) AND (hd_vehicle_count#9 <= 6)) OR ((hd_dep_count#8 = 2) AND (hd_vehicle_count#9 <= 4))) OR ((hd_dep_count#8 = 0) AND (hd_vehicle_count#9 <= 2))) AND isnotnull(hd_demo_sk#7))

(15) Project [codegen id : 2]
Output [1]: [hd_demo_sk#7]
Input [3]: [hd_demo_sk#7, hd_dep_count#8, hd_vehicle_count#9]

(16) BroadcastExchange
Input [1]: [hd_demo_sk#7]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=2]

(17) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_hdemo_sk#2]
Right keys [1]: [hd_demo_sk#7]
Join type: Inner
Join condition: None

(18) Project [codegen id : 4]
Output [1]: [ss_sold_time_sk#1]
Input [3]: [ss_sold_time_sk#1, ss_hdemo_sk#2, hd_demo_sk#7]

(19) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#10, t_hour#11, t_minute#12]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,8), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(20) ColumnarToRow [codegen id : 3]
Input [3]: [t_time_sk#10, t_hour#11, t_minute#12]

(21) Filter [codegen id : 3]
Input [3]: [t_time_sk#10, t_hour#11, t_minute#12]
Condition : ((((isnotnull(t_hour#11) AND isnotnull(t_minute#12)) AND (t_hour#11 = 8)) AND (t_minute#12 >= 30)) AND isnotnull(t_time_sk#10))

(22) Project [codegen id : 3]
Output [1]: [t_time_sk#10]
Input [3]: [t_time_sk#10, t_hour#11, t_minute#12]

(23) BroadcastExchange
Input [1]: [t_time_sk#10]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=3]

(24) BroadcastHashJoin [codegen id : 4]
Left keys [1]: [ss_sold_time_sk#1]
Right keys [1]: [t_time_sk#10]
Join type: Inner
Join condition: None

(25) Project [codegen id : 4]
Output: []
Input [2]: [ss_sold_time_sk#1, t_time_sk#10]

(26) HashAggregate [codegen id : 4]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#13]
Results [1]: [count#14]

(27) Exchange
Input [1]: [count#14]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=4]

(28) HashAggregate [codegen id : 40]
Input [1]: [count#14]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#15]
Results [1]: [count(1)#15 AS h8_30_to_9#16]

(29) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#17, ss_hdemo_sk#18, ss_store_sk#19, ss_sold_date_sk#20]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(30) ColumnarToRow [codegen id : 8]
Input [4]: [ss_sold_time_sk#17, ss_hdemo_sk#18, ss_store_sk#19, ss_sold_date_sk#20]

(31) Filter [codegen id : 8]
Input [4]: [ss_sold_time_sk#17, ss_hdemo_sk#18, ss_store_sk#19, ss_sold_date_sk#20]
Condition : ((isnotnull(ss_hdemo_sk#18) AND isnotnull(ss_sold_time_sk#17)) AND isnotnull(ss_store_sk#19))

(32) Project [codegen id : 8]
Output [3]: [ss_sold_time_sk#17, ss_hdemo_sk#18, ss_store_sk#19]
Input [4]: [ss_sold_time_sk#17, ss_hdemo_sk#18, ss_store_sk#19, ss_sold_date_sk#20]

(33) ReusedExchange [Reuses operator id: 9]
Output [1]: [s_store_sk#21]

(34) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_store_sk#19]
Right keys [1]: [s_store_sk#21]
Join type: Inner
Join condition: None

(35) Project [codegen id : 8]
Output [2]: [ss_sold_time_sk#17, ss_hdemo_sk#18]
Input [4]: [ss_sold_time_sk#17, ss_hdemo_sk#18, ss_store_sk#19, s_store_sk#21]

(36) ReusedExchange [Reuses operator id: 16]
Output [1]: [hd_demo_sk#22]

(37) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_hdemo_sk#18]
Right keys [1]: [hd_demo_sk#22]
Join type: Inner
Join condition: None

(38) Project [codegen id : 8]
Output [1]: [ss_sold_time_sk#17]
Input [3]: [ss_sold_time_sk#17, ss_hdemo_sk#18, hd_demo_sk#22]

(39) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#23, t_hour#24, t_minute#25]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(40) ColumnarToRow [codegen id : 7]
Input [3]: [t_time_sk#23, t_hour#24, t_minute#25]

(41) Filter [codegen id : 7]
Input [3]: [t_time_sk#23, t_hour#24, t_minute#25]
Condition : ((((isnotnull(t_hour#24) AND isnotnull(t_minute#25)) AND (t_hour#24 = 9)) AND (t_minute#25 < 30)) AND isnotnull(t_time_sk#23))

(42) Project [codegen id : 7]
Output [1]: [t_time_sk#23]
Input [3]: [t_time_sk#23, t_hour#24, t_minute#25]

(43) BroadcastExchange
Input [1]: [t_time_sk#23]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=5]

(44) BroadcastHashJoin [codegen id : 8]
Left keys [1]: [ss_sold_time_sk#17]
Right keys [1]: [t_time_sk#23]
Join type: Inner
Join condition: None

(45) Project [codegen id : 8]
Output: []
Input [2]: [ss_sold_time_sk#17, t_time_sk#23]

(46) HashAggregate [codegen id : 8]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#26]
Results [1]: [count#27]

(47) Exchange
Input [1]: [count#27]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=6]

(48) HashAggregate [codegen id : 9]
Input [1]: [count#27]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#28]
Results [1]: [count(1)#28 AS h9_to_9_30#29]

(49) BroadcastExchange
Input [1]: [h9_to_9_30#29]
Arguments: IdentityBroadcastMode, [plan_id=7]

(50) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(51) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#30, ss_hdemo_sk#31, ss_store_sk#32, ss_sold_date_sk#33]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(52) ColumnarToRow [codegen id : 13]
Input [4]: [ss_sold_time_sk#30, ss_hdemo_sk#31, ss_store_sk#32, ss_sold_date_sk#33]

(53) Filter [codegen id : 13]
Input [4]: [ss_sold_time_sk#30, ss_hdemo_sk#31, ss_store_sk#32, ss_sold_date_sk#33]
Condition : ((isnotnull(ss_hdemo_sk#31) AND isnotnull(ss_sold_time_sk#30)) AND isnotnull(ss_store_sk#32))

(54) Project [codegen id : 13]
Output [3]: [ss_sold_time_sk#30, ss_hdemo_sk#31, ss_store_sk#32]
Input [4]: [ss_sold_time_sk#30, ss_hdemo_sk#31, ss_store_sk#32, ss_sold_date_sk#33]

(55) ReusedExchange [Reuses operator id: 9]
Output [1]: [s_store_sk#34]

(56) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ss_store_sk#32]
Right keys [1]: [s_store_sk#34]
Join type: Inner
Join condition: None

(57) Project [codegen id : 13]
Output [2]: [ss_sold_time_sk#30, ss_hdemo_sk#31]
Input [4]: [ss_sold_time_sk#30, ss_hdemo_sk#31, ss_store_sk#32, s_store_sk#34]

(58) ReusedExchange [Reuses operator id: 16]
Output [1]: [hd_demo_sk#35]

(59) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ss_hdemo_sk#31]
Right keys [1]: [hd_demo_sk#35]
Join type: Inner
Join condition: None

(60) Project [codegen id : 13]
Output [1]: [ss_sold_time_sk#30]
Input [3]: [ss_sold_time_sk#30, ss_hdemo_sk#31, hd_demo_sk#35]

(61) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#36, t_hour#37, t_minute#38]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,9), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(62) ColumnarToRow [codegen id : 12]
Input [3]: [t_time_sk#36, t_hour#37, t_minute#38]

(63) Filter [codegen id : 12]
Input [3]: [t_time_sk#36, t_hour#37, t_minute#38]
Condition : ((((isnotnull(t_hour#37) AND isnotnull(t_minute#38)) AND (t_hour#37 = 9)) AND (t_minute#38 >= 30)) AND isnotnull(t_time_sk#36))

(64) Project [codegen id : 12]
Output [1]: [t_time_sk#36]
Input [3]: [t_time_sk#36, t_hour#37, t_minute#38]

(65) BroadcastExchange
Input [1]: [t_time_sk#36]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=8]

(66) BroadcastHashJoin [codegen id : 13]
Left keys [1]: [ss_sold_time_sk#30]
Right keys [1]: [t_time_sk#36]
Join type: Inner
Join condition: None

(67) Project [codegen id : 13]
Output: []
Input [2]: [ss_sold_time_sk#30, t_time_sk#36]

(68) HashAggregate [codegen id : 13]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#39]
Results [1]: [count#40]

(69) Exchange
Input [1]: [count#40]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=9]

(70) HashAggregate [codegen id : 14]
Input [1]: [count#40]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#41]
Results [1]: [count(1)#41 AS h9_30_to_10#42]

(71) BroadcastExchange
Input [1]: [h9_30_to_10#42]
Arguments: IdentityBroadcastMode, [plan_id=10]

(72) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(73) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#43, ss_hdemo_sk#44, ss_store_sk#45, ss_sold_date_sk#46]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(74) ColumnarToRow [codegen id : 18]
Input [4]: [ss_sold_time_sk#43, ss_hdemo_sk#44, ss_store_sk#45, ss_sold_date_sk#46]

(75) Filter [codegen id : 18]
Input [4]: [ss_sold_time_sk#43, ss_hdemo_sk#44, ss_store_sk#45, ss_sold_date_sk#46]
Condition : ((isnotnull(ss_hdemo_sk#44) AND isnotnull(ss_sold_time_sk#43)) AND isnotnull(ss_store_sk#45))

(76) Project [codegen id : 18]
Output [3]: [ss_sold_time_sk#43, ss_hdemo_sk#44, ss_store_sk#45]
Input [4]: [ss_sold_time_sk#43, ss_hdemo_sk#44, ss_store_sk#45, ss_sold_date_sk#46]

(77) ReusedExchange [Reuses operator id: 9]
Output [1]: [s_store_sk#47]

(78) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ss_store_sk#45]
Right keys [1]: [s_store_sk#47]
Join type: Inner
Join condition: None

(79) Project [codegen id : 18]
Output [2]: [ss_sold_time_sk#43, ss_hdemo_sk#44]
Input [4]: [ss_sold_time_sk#43, ss_hdemo_sk#44, ss_store_sk#45, s_store_sk#47]

(80) ReusedExchange [Reuses operator id: 16]
Output [1]: [hd_demo_sk#48]

(81) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ss_hdemo_sk#44]
Right keys [1]: [hd_demo_sk#48]
Join type: Inner
Join condition: None

(82) Project [codegen id : 18]
Output [1]: [ss_sold_time_sk#43]
Input [3]: [ss_sold_time_sk#43, ss_hdemo_sk#44, hd_demo_sk#48]

(83) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#49, t_hour#50, t_minute#51]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(84) ColumnarToRow [codegen id : 17]
Input [3]: [t_time_sk#49, t_hour#50, t_minute#51]

(85) Filter [codegen id : 17]
Input [3]: [t_time_sk#49, t_hour#50, t_minute#51]
Condition : ((((isnotnull(t_hour#50) AND isnotnull(t_minute#51)) AND (t_hour#50 = 10)) AND (t_minute#51 < 30)) AND isnotnull(t_time_sk#49))

(86) Project [codegen id : 17]
Output [1]: [t_time_sk#49]
Input [3]: [t_time_sk#49, t_hour#50, t_minute#51]

(87) BroadcastExchange
Input [1]: [t_time_sk#49]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=11]

(88) BroadcastHashJoin [codegen id : 18]
Left keys [1]: [ss_sold_time_sk#43]
Right keys [1]: [t_time_sk#49]
Join type: Inner
Join condition: None

(89) Project [codegen id : 18]
Output: []
Input [2]: [ss_sold_time_sk#43, t_time_sk#49]

(90) HashAggregate [codegen id : 18]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#52]
Results [1]: [count#53]

(91) Exchange
Input [1]: [count#53]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=12]

(92) HashAggregate [codegen id : 19]
Input [1]: [count#53]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#54]
Results [1]: [count(1)#54 AS h10_to_10_30#55]

(93) BroadcastExchange
Input [1]: [h10_to_10_30#55]
Arguments: IdentityBroadcastMode, [plan_id=13]

(94) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(95) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#56, ss_hdemo_sk#57, ss_store_sk#58, ss_sold_date_sk#59]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(96) ColumnarToRow [codegen id : 23]
Input [4]: [ss_sold_time_sk#56, ss_hdemo_sk#57, ss_store_sk#58, ss_sold_date_sk#59]

(97) Filter [codegen id : 23]
Input [4]: [ss_sold_time_sk#56, ss_hdemo_sk#57, ss_store_sk#58, ss_sold_date_sk#59]
Condition : ((isnotnull(ss_hdemo_sk#57) AND isnotnull(ss_sold_time_sk#56)) AND isnotnull(ss_store_sk#58))

(98) Project [codegen id : 23]
Output [3]: [ss_sold_time_sk#56, ss_hdemo_sk#57, ss_store_sk#58]
Input [4]: [ss_sold_time_sk#56, ss_hdemo_sk#57, ss_store_sk#58, ss_sold_date_sk#59]

(99) ReusedExchange [Reuses operator id: 9]
Output [1]: [s_store_sk#60]

(100) BroadcastHashJoin [codegen id : 23]
Left keys [1]: [ss_store_sk#58]
Right keys [1]: [s_store_sk#60]
Join type: Inner
Join condition: None

(101) Project [codegen id : 23]
Output [2]: [ss_sold_time_sk#56, ss_hdemo_sk#57]
Input [4]: [ss_sold_time_sk#56, ss_hdemo_sk#57, ss_store_sk#58, s_store_sk#60]

(102) ReusedExchange [Reuses operator id: 16]
Output [1]: [hd_demo_sk#61]

(103) BroadcastHashJoin [codegen id : 23]
Left keys [1]: [ss_hdemo_sk#57]
Right keys [1]: [hd_demo_sk#61]
Join type: Inner
Join condition: None

(104) Project [codegen id : 23]
Output [1]: [ss_sold_time_sk#56]
Input [3]: [ss_sold_time_sk#56, ss_hdemo_sk#57, hd_demo_sk#61]

(105) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#62, t_hour#63, t_minute#64]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,10), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(106) ColumnarToRow [codegen id : 22]
Input [3]: [t_time_sk#62, t_hour#63, t_minute#64]

(107) Filter [codegen id : 22]
Input [3]: [t_time_sk#62, t_hour#63, t_minute#64]
Condition : ((((isnotnull(t_hour#63) AND isnotnull(t_minute#64)) AND (t_hour#63 = 10)) AND (t_minute#64 >= 30)) AND isnotnull(t_time_sk#62))

(108) Project [codegen id : 22]
Output [1]: [t_time_sk#62]
Input [3]: [t_time_sk#62, t_hour#63, t_minute#64]

(109) BroadcastExchange
Input [1]: [t_time_sk#62]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=14]

(110) BroadcastHashJoin [codegen id : 23]
Left keys [1]: [ss_sold_time_sk#56]
Right keys [1]: [t_time_sk#62]
Join type: Inner
Join condition: None

(111) Project [codegen id : 23]
Output: []
Input [2]: [ss_sold_time_sk#56, t_time_sk#62]

(112) HashAggregate [codegen id : 23]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#65]
Results [1]: [count#66]

(113) Exchange
Input [1]: [count#66]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=15]

(114) HashAggregate [codegen id : 24]
Input [1]: [count#66]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#67]
Results [1]: [count(1)#67 AS h10_30_to_11#68]

(115) BroadcastExchange
Input [1]: [h10_30_to_11#68]
Arguments: IdentityBroadcastMode, [plan_id=16]

(116) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(117) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#69, ss_hdemo_sk#70, ss_store_sk#71, ss_sold_date_sk#72]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(118) ColumnarToRow [codegen id : 28]
Input [4]: [ss_sold_time_sk#69, ss_hdemo_sk#70, ss_store_sk#71, ss_sold_date_sk#72]

(119) Filter [codegen id : 28]
Input [4]: [ss_sold_time_sk#69, ss_hdemo_sk#70, ss_store_sk#71, ss_sold_date_sk#72]
Condition : ((isnotnull(ss_hdemo_sk#70) AND isnotnull(ss_sold_time_sk#69)) AND isnotnull(ss_store_sk#71))

(120) Project [codegen id : 28]
Output [3]: [ss_sold_time_sk#69, ss_hdemo_sk#70, ss_store_sk#71]
Input [4]: [ss_sold_time_sk#69, ss_hdemo_sk#70, ss_store_sk#71, ss_sold_date_sk#72]

(121) ReusedExchange [Reuses operator id: 9]
Output [1]: [s_store_sk#73]

(122) BroadcastHashJoin [codegen id : 28]
Left keys [1]: [ss_store_sk#71]
Right keys [1]: [s_store_sk#73]
Join type: Inner
Join condition: None

(123) Project [codegen id : 28]
Output [2]: [ss_sold_time_sk#69, ss_hdemo_sk#70]
Input [4]: [ss_sold_time_sk#69, ss_hdemo_sk#70, ss_store_sk#71, s_store_sk#73]

(124) ReusedExchange [Reuses operator id: 16]
Output [1]: [hd_demo_sk#74]

(125) BroadcastHashJoin [codegen id : 28]
Left keys [1]: [ss_hdemo_sk#70]
Right keys [1]: [hd_demo_sk#74]
Join type: Inner
Join condition: None

(126) Project [codegen id : 28]
Output [1]: [ss_sold_time_sk#69]
Input [3]: [ss_sold_time_sk#69, ss_hdemo_sk#70, hd_demo_sk#74]

(127) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#75, t_hour#76, t_minute#77]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(128) ColumnarToRow [codegen id : 27]
Input [3]: [t_time_sk#75, t_hour#76, t_minute#77]

(129) Filter [codegen id : 27]
Input [3]: [t_time_sk#75, t_hour#76, t_minute#77]
Condition : ((((isnotnull(t_hour#76) AND isnotnull(t_minute#77)) AND (t_hour#76 = 11)) AND (t_minute#77 < 30)) AND isnotnull(t_time_sk#75))

(130) Project [codegen id : 27]
Output [1]: [t_time_sk#75]
Input [3]: [t_time_sk#75, t_hour#76, t_minute#77]

(131) BroadcastExchange
Input [1]: [t_time_sk#75]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=17]

(132) BroadcastHashJoin [codegen id : 28]
Left keys [1]: [ss_sold_time_sk#69]
Right keys [1]: [t_time_sk#75]
Join type: Inner
Join condition: None

(133) Project [codegen id : 28]
Output: []
Input [2]: [ss_sold_time_sk#69, t_time_sk#75]

(134) HashAggregate [codegen id : 28]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#78]
Results [1]: [count#79]

(135) Exchange
Input [1]: [count#79]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=18]

(136) HashAggregate [codegen id : 29]
Input [1]: [count#79]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#80]
Results [1]: [count(1)#80 AS h11_to_11_30#81]

(137) BroadcastExchange
Input [1]: [h11_to_11_30#81]
Arguments: IdentityBroadcastMode, [plan_id=19]

(138) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(139) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#82, ss_hdemo_sk#83, ss_store_sk#84, ss_sold_date_sk#85]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(140) ColumnarToRow [codegen id : 33]
Input [4]: [ss_sold_time_sk#82, ss_hdemo_sk#83, ss_store_sk#84, ss_sold_date_sk#85]

(141) Filter [codegen id : 33]
Input [4]: [ss_sold_time_sk#82, ss_hdemo_sk#83, ss_store_sk#84, ss_sold_date_sk#85]
Condition : ((isnotnull(ss_hdemo_sk#83) AND isnotnull(ss_sold_time_sk#82)) AND isnotnull(ss_store_sk#84))

(142) Project [codegen id : 33]
Output [3]: [ss_sold_time_sk#82, ss_hdemo_sk#83, ss_store_sk#84]
Input [4]: [ss_sold_time_sk#82, ss_hdemo_sk#83, ss_store_sk#84, ss_sold_date_sk#85]

(143) ReusedExchange [Reuses operator id: 9]
Output [1]: [s_store_sk#86]

(144) BroadcastHashJoin [codegen id : 33]
Left keys [1]: [ss_store_sk#84]
Right keys [1]: [s_store_sk#86]
Join type: Inner
Join condition: None

(145) Project [codegen id : 33]
Output [2]: [ss_sold_time_sk#82, ss_hdemo_sk#83]
Input [4]: [ss_sold_time_sk#82, ss_hdemo_sk#83, ss_store_sk#84, s_store_sk#86]

(146) ReusedExchange [Reuses operator id: 16]
Output [1]: [hd_demo_sk#87]

(147) BroadcastHashJoin [codegen id : 33]
Left keys [1]: [ss_hdemo_sk#83]
Right keys [1]: [hd_demo_sk#87]
Join type: Inner
Join condition: None

(148) Project [codegen id : 33]
Output [1]: [ss_sold_time_sk#82]
Input [3]: [ss_sold_time_sk#82, ss_hdemo_sk#83, hd_demo_sk#87]

(149) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#88, t_hour#89, t_minute#90]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,11), GreaterThanOrEqual(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(150) ColumnarToRow [codegen id : 32]
Input [3]: [t_time_sk#88, t_hour#89, t_minute#90]

(151) Filter [codegen id : 32]
Input [3]: [t_time_sk#88, t_hour#89, t_minute#90]
Condition : ((((isnotnull(t_hour#89) AND isnotnull(t_minute#90)) AND (t_hour#89 = 11)) AND (t_minute#90 >= 30)) AND isnotnull(t_time_sk#88))

(152) Project [codegen id : 32]
Output [1]: [t_time_sk#88]
Input [3]: [t_time_sk#88, t_hour#89, t_minute#90]

(153) BroadcastExchange
Input [1]: [t_time_sk#88]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=20]

(154) BroadcastHashJoin [codegen id : 33]
Left keys [1]: [ss_sold_time_sk#82]
Right keys [1]: [t_time_sk#88]
Join type: Inner
Join condition: None

(155) Project [codegen id : 33]
Output: []
Input [2]: [ss_sold_time_sk#82, t_time_sk#88]

(156) HashAggregate [codegen id : 33]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#91]
Results [1]: [count#92]

(157) Exchange
Input [1]: [count#92]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=21]

(158) HashAggregate [codegen id : 34]
Input [1]: [count#92]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#93]
Results [1]: [count(1)#93 AS h11_30_to_12#94]

(159) BroadcastExchange
Input [1]: [h11_30_to_12#94]
Arguments: IdentityBroadcastMode, [plan_id=22]

(160) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

(161) Scan parquet spark_catalog.default.store_sales
Output [4]: [ss_sold_time_sk#95, ss_hdemo_sk#96, ss_store_sk#97, ss_sold_date_sk#98]
Batched: true
Location [not included in comparison]/{warehouse_dir}/store_sales]
PushedFilters: [IsNotNull(ss_hdemo_sk), IsNotNull(ss_sold_time_sk), IsNotNull(ss_store_sk)]
ReadSchema: struct<ss_sold_time_sk:int,ss_hdemo_sk:int,ss_store_sk:int>

(162) ColumnarToRow [codegen id : 38]
Input [4]: [ss_sold_time_sk#95, ss_hdemo_sk#96, ss_store_sk#97, ss_sold_date_sk#98]

(163) Filter [codegen id : 38]
Input [4]: [ss_sold_time_sk#95, ss_hdemo_sk#96, ss_store_sk#97, ss_sold_date_sk#98]
Condition : ((isnotnull(ss_hdemo_sk#96) AND isnotnull(ss_sold_time_sk#95)) AND isnotnull(ss_store_sk#97))

(164) Project [codegen id : 38]
Output [3]: [ss_sold_time_sk#95, ss_hdemo_sk#96, ss_store_sk#97]
Input [4]: [ss_sold_time_sk#95, ss_hdemo_sk#96, ss_store_sk#97, ss_sold_date_sk#98]

(165) ReusedExchange [Reuses operator id: 9]
Output [1]: [s_store_sk#99]

(166) BroadcastHashJoin [codegen id : 38]
Left keys [1]: [ss_store_sk#97]
Right keys [1]: [s_store_sk#99]
Join type: Inner
Join condition: None

(167) Project [codegen id : 38]
Output [2]: [ss_sold_time_sk#95, ss_hdemo_sk#96]
Input [4]: [ss_sold_time_sk#95, ss_hdemo_sk#96, ss_store_sk#97, s_store_sk#99]

(168) ReusedExchange [Reuses operator id: 16]
Output [1]: [hd_demo_sk#100]

(169) BroadcastHashJoin [codegen id : 38]
Left keys [1]: [ss_hdemo_sk#96]
Right keys [1]: [hd_demo_sk#100]
Join type: Inner
Join condition: None

(170) Project [codegen id : 38]
Output [1]: [ss_sold_time_sk#95]
Input [3]: [ss_sold_time_sk#95, ss_hdemo_sk#96, hd_demo_sk#100]

(171) Scan parquet spark_catalog.default.time_dim
Output [3]: [t_time_sk#101, t_hour#102, t_minute#103]
Batched: true
Location [not included in comparison]/{warehouse_dir}/time_dim]
PushedFilters: [IsNotNull(t_hour), IsNotNull(t_minute), EqualTo(t_hour,12), LessThan(t_minute,30), IsNotNull(t_time_sk)]
ReadSchema: struct<t_time_sk:int,t_hour:int,t_minute:int>

(172) ColumnarToRow [codegen id : 37]
Input [3]: [t_time_sk#101, t_hour#102, t_minute#103]

(173) Filter [codegen id : 37]
Input [3]: [t_time_sk#101, t_hour#102, t_minute#103]
Condition : ((((isnotnull(t_hour#102) AND isnotnull(t_minute#103)) AND (t_hour#102 = 12)) AND (t_minute#103 < 30)) AND isnotnull(t_time_sk#101))

(174) Project [codegen id : 37]
Output [1]: [t_time_sk#101]
Input [3]: [t_time_sk#101, t_hour#102, t_minute#103]

(175) BroadcastExchange
Input [1]: [t_time_sk#101]
Arguments: HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [plan_id=23]

(176) BroadcastHashJoin [codegen id : 38]
Left keys [1]: [ss_sold_time_sk#95]
Right keys [1]: [t_time_sk#101]
Join type: Inner
Join condition: None

(177) Project [codegen id : 38]
Output: []
Input [2]: [ss_sold_time_sk#95, t_time_sk#101]

(178) HashAggregate [codegen id : 38]
Input: []
Keys: []
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#104]
Results [1]: [count#105]

(179) Exchange
Input [1]: [count#105]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=24]

(180) HashAggregate [codegen id : 39]
Input [1]: [count#105]
Keys: []
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#106]
Results [1]: [count(1)#106 AS h12_to_12_30#107]

(181) BroadcastExchange
Input [1]: [h12_to_12_30#107]
Arguments: IdentityBroadcastMode, [plan_id=25]

(182) BroadcastNestedLoopJoin [codegen id : 40]
Join type: Inner
Join condition: None

